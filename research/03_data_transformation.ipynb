{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65f93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e938ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ARSH\\\\Desktop\\\\MLOps\\\\Text-Summerization\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef54a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44d6dd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ARSH\\\\Desktop\\\\MLOps\\\\Text-Summerization'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e54023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    tokenizer_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c27def88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurations for data transformation\n",
    "from textSummaizer.constants import *\n",
    "from textSummaizer.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d884303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self,\n",
    "            config_filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            self.params = read_yaml(params_filepath)\n",
    "\n",
    "            create_directories([self.config.artifacts_root])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config= self.config.data_transformation\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            tokenizer_name=config.tokenizer_name\n",
    "        )\n",
    "        return data_transformation_config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26568b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textSummaizer.logging import logger\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset,load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39ff965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_name)\n",
    "        logger.info(f\"Tokenizer loaded from {self.config.tokenizer_name}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    def convert_example_to_features(self,example_batch):\n",
    "        input_encodings = self.tokenizer(\n",
    "            example_batch['dialogue'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=1024,\n",
    "        )\n",
    "        \n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            target_encodings = self.tokenizer(\n",
    "                example_batch['summary'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=128,\n",
    "            )\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "        \n",
    "    def convert(self):\n",
    "        dataset_samsum = load_from_disk(self.config.data_path)\n",
    "        dataset_samsum_pt= dataset_samsum.map(\n",
    "            self.convert_example_to_features,\n",
    "            batched=True,\n",
    "        )\n",
    "        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir, \"samsum_dataset\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f85441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-29 20:02:05,916 - textSummarizerlogger - INFO - YAML file config\\config.yaml read successfully.]\n",
      "[2025-07-29 20:02:05,916 - textSummarizerlogger - INFO - YAML file params.yaml read successfully.]\n",
      "[2025-07-29 20:02:05,916 - textSummarizerlogger - INFO - Created directory: artifacts]\n",
      "[2025-07-29 20:02:05,916 - textSummarizerlogger - INFO - Created directory: artifacts/data_transformation]\n",
      "[2025-07-29 20:02:07,453 - textSummarizerlogger - INFO - Tokenizer loaded from t5-small]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3951: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 14732/14732 [00:04<00:00, 3671.69 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 3680.24 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 3707.81 examples/s]\n",
      "Saving the dataset (0/1 shards):   0%|          | 0/14732 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-29 20:02:12,000 - textSummarizerlogger - ERROR - [Errno 22] Invalid argument: 'c:/Users/ARSH/Desktop/MLOps/Text-Summerization/artifacts/data_transformation/samsum_dataset/train/data-00000-of-00001.arrow']\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ARSH\\AppData\\Local\\Temp\\ipykernel_11524\\1664774545.py\", line 6, in <module>\n",
      "    data_transformation.convert()\n",
      "  File \"C:\\Users\\ARSH\\AppData\\Local\\Temp\\ipykernel_11524\\1656508627.py\", line 36, in convert\n",
      "    dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir, \"samsum_dataset\"))\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\dataset_dict.py\", line 1350, in save_to_disk\n",
      "    dataset.save_to_disk(\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 1626, in save_to_disk\n",
      "    for job_id, done, content in Dataset._save_to_disk_single(**kwargs):\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 1650, in _save_to_disk_single\n",
      "    writer = ArrowWriter(\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\arrow_writer.py\", line 389, in __init__\n",
      "    self.stream = self._fs.open(path, \"wb\")\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\spec.py\", line 1310, in open\n",
      "    f = self._open(\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 201, in _open\n",
      "    return LocalFileOpener(path, mode, fs=self, **kwargs)\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 365, in __init__\n",
      "    self._open()\n",
      "  File \"c:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\implementations\\local.py\", line 370, in _open\n",
      "    self.f = open(self.path, mode=self.mode)\n",
      "OSError: [Errno 22] Invalid argument: 'c:/Users/ARSH/Desktop/MLOps/Text-Summerization/artifacts/data_transformation/samsum_dataset/train/data-00000-of-00001.arrow'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'c:/Users/ARSH/Desktop/MLOps/Text-Summerization/artifacts/data_transformation/samsum_dataset/train/data-00000-of-00001.arrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      8\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(e)\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     data_transformation_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_data_transformation_config()\n\u001b[0;32m      5\u001b[0m     data_transformation \u001b[38;5;241m=\u001b[39m DataTransformation(config\u001b[38;5;241m=\u001b[39mdata_transformation_config)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43mdata_transformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      8\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(e)\n",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m, in \u001b[0;36mDataTransformation.convert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m dataset_samsum \u001b[38;5;241m=\u001b[39m load_from_disk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdata_path)\n\u001b[0;32m     32\u001b[0m dataset_samsum_pt\u001b[38;5;241m=\u001b[39m dataset_samsum\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_example_to_features,\n\u001b[0;32m     34\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m )\n\u001b[1;32m---> 36\u001b[0m \u001b[43mdataset_samsum_pt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msamsum_dataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\dataset_dict.py:1350\u001b[0m, in \u001b[0;36mDatasetDict.save_to_disk\u001b[1;34m(self, dataset_dict_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)}, f)\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m-> 1350\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposixpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_shard_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_shard_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:1626\u001b[0m, in \u001b[0;36mDataset.save_to_disk\u001b[1;34m(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m kwargs \u001b[38;5;129;01min\u001b[39;00m kwargs_per_job:\n\u001b[1;32m-> 1626\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_save_to_disk_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1627\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   1628\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\arrow_dataset.py:1650\u001b[0m, in \u001b[0;36mDataset._save_to_disk_single\u001b[1;34m(job_id, shard, fpath, storage_options)\u001b[0m\n\u001b[0;32m   1647\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mDEFAULT_MAX_BATCH_SIZE\n\u001b[0;32m   1649\u001b[0m num_examples_progress_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1650\u001b[0m writer \u001b[38;5;241m=\u001b[39m \u001b[43mArrowWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43membed_local_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1655\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1656\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1657\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\datasets\\arrow_writer.py:389\u001b[0m, in \u001b[0;36mArrowWriter.__init__\u001b[1;34m(self, schema, features, path, stream, fingerprint, writer_batch_size, hash_salt, check_duplicates, disable_nullable, update_features, with_metadata, unit, embed_local_files, storage_options)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs: fsspec\u001b[38;5;241m.\u001b[39mAbstractFileSystem \u001b[38;5;241m=\u001b[39m fs\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m path \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_filesystem(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39munstrip_protocol(path)\n\u001b[1;32m--> 389\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closable_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\spec.py:1310\u001b[0m, in \u001b[0;36mAbstractFileSystem.open\u001b[1;34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     ac \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautocommit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intrans)\n\u001b[1;32m-> 1310\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(\n\u001b[0;32m   1311\u001b[0m         path,\n\u001b[0;32m   1312\u001b[0m         mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   1313\u001b[0m         block_size\u001b[38;5;241m=\u001b[39mblock_size,\n\u001b[0;32m   1314\u001b[0m         autocommit\u001b[38;5;241m=\u001b[39mac,\n\u001b[0;32m   1315\u001b[0m         cache_options\u001b[38;5;241m=\u001b[39mcache_options,\n\u001b[0;32m   1316\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1317\u001b[0m     )\n\u001b[0;32m   1318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1319\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compr\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\implementations\\local.py:201\u001b[0m, in \u001b[0;36mLocalFileSystem._open\u001b[1;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_mkdir \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent(path), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalFileOpener(path, mode, fs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\implementations\\local.py:365\u001b[0m, in \u001b[0;36mLocalFileOpener.__init__\u001b[1;34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m get_compression(path, compression)\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[1;32m--> 365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ARSH\\Desktop\\MLOps\\Text-Summerization\\.venv\\lib\\site-packages\\fsspec\\implementations\\local.py:370\u001b[0m, in \u001b[0;36mLocalFileOpener._open\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mclosed:\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m--> 370\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression:\n\u001b[0;32m    372\u001b[0m             compress \u001b[38;5;241m=\u001b[39m compr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression]\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'c:/Users/ARSH/Desktop/MLOps/Text-Summerization/artifacts/data_transformation/samsum_dataset/train/data-00000-of-00001.arrow'"
     ]
    }
   ],
   "source": [
    "#pipeline\n",
    "try:\n",
    "    config= ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.convert()\n",
    "except Exception as e:\n",
    "    logger.exception(e)\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ff0318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
